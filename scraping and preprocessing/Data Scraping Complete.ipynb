{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0b31d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ebdca4",
   "metadata": {},
   "source": [
    "### Scraping from FBRef Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67910851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website to scrape\n",
    "url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "\n",
    "# Send an HTTP GET request to the website and parse it\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an array of the teams url and the teams name\n",
    "teams = []\n",
    "teams_name=[]\n",
    "href='https://fbref.com/'\n",
    "for team in soup.find_all(\"td\", attrs={\"data-stat\": \"team\"}):\n",
    "    team_link = team.find('a')\n",
    "    team_href = team_link.get(\"href\")\n",
    "    team_href = href+team_href\n",
    "    teams.append(team_href)\n",
    "    teams_name.append(team_link.text)\n",
    "time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01da7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get a dataframe for the player stats\n",
    "\n",
    "def merge_levels(column):\n",
    "    col = column.split('?')\n",
    "    if col[0] == 'Per 90 Minutes':\n",
    "        return col[1] + '/90'\n",
    "    else:        \n",
    "        return col[1]\n",
    "\n",
    "def get_team_stats(url,team_name,table_tag):\n",
    "    # Retrieve HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the team stats\n",
    "    table_tags = soup.find_all('table', attrs={'id': table_tag})\n",
    "    if not table_tags:\n",
    "        print(\"No table found with id 'stats_standard_9'.\")\n",
    "        return None\n",
    "\n",
    "    # Read the table into a DataFrame\n",
    "    df = pd.read_html(str(table_tags[0]))[0]\n",
    "\n",
    "    # Merge MultiIndex levels and drop 'Matches' column\n",
    "    df.columns = df.columns.map('?'.join)\n",
    "    df.columns = df.columns.map(merge_levels)\n",
    "    df.drop('Matches', axis=1, inplace=True)\n",
    "\n",
    "    # Extract team name from URL\n",
    "    df['Team'] = team_name\n",
    "    \n",
    "    df = df[~df['Player'].isin(['Opponent Total', 'Squad Total'])]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example function call\n",
    "df1 = get_team_stats(teams[0],teams_name[0],'stats_shooting_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b117aed",
   "metadata": {},
   "source": [
    "\n",
    "I have to gather the data for each statistic manually because if I try to automate the process and make too many requests in a loop, the website blocks me from scraping more data. So, I have to wait a few seconds between each request before continuing to gather the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb8f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the shooting stats dataframe\n",
    "\n",
    "df_teams = pd.DataFrame()\n",
    "\n",
    "i=0\n",
    "for team in teams:\n",
    "    df_team = get_team_stats(team,teams_name[i],'stats_shooting_9')\n",
    "    df_teams = pd.concat([df_teams, df_team], ignore_index=True)\n",
    "    print(team)\n",
    "    i=i+1\n",
    "    time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b289ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams.to_csv('shooting_stat.csv',index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d570ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the passing stats dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_teams = pd.DataFrame()\n",
    "\n",
    "i=0\n",
    "for team in teams:\n",
    "    df_team = get_team_stats(team,teams_name[i],'stats_passing_9')\n",
    "    df_teams = pd.concat([df_teams, df_team], ignore_index=True)\n",
    "    print(team)\n",
    "    i=i+1\n",
    "    time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649adc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams.to_csv('passing_stats.csv',index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gca stats dataframe\n",
    "\n",
    "df_teams = pd.DataFrame()\n",
    "\n",
    "i=0\n",
    "for team in teams:\n",
    "    df_team = get_team_stats(team,teams_name[i],'stats_gca_9')\n",
    "    df_teams = pd.concat([df_teams, df_team], ignore_index=True)\n",
    "    print(team)\n",
    "    i=i+1\n",
    "    time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams.to_csv('gca_stats.csv',index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ec93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the defense stats dataframe\n",
    "\n",
    "df_teams = pd.DataFrame()\n",
    "\n",
    "i=0\n",
    "for team in teams:\n",
    "    df_team = get_team_stats(team,teams_name[i],'stats_defense_9')\n",
    "    df_teams = pd.concat([df_teams, df_team], ignore_index=True)\n",
    "    print(team)\n",
    "    i=i+1\n",
    "    time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams.to_csv('gca_stats.csv',index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e854b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the possession stats dataframe\n",
    "\n",
    "df_teams = pd.DataFrame()\n",
    "\n",
    "i=0\n",
    "for team in teams:\n",
    "    df_team = get_team_stats(team,teams_name[i],'stats_possession_9')\n",
    "    df_teams = pd.concat([df_teams, df_team], ignore_index=True)\n",
    "    print(team)\n",
    "    i=i+1\n",
    "    time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b01dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams.to_csv('possession_stats.csv',index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279741f9",
   "metadata": {},
   "source": [
    "Considering the model's performance, the Mean Absolute Error (MEA) metric is significantly large. Hence, I'm attempting to scrape data on player wages, as it could prove useful in predicting player valuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab76789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website to scrape\n",
    "url = \"https://fbref.com/en/comps/9/wages/Premier-League-Wages\"\n",
    "\n",
    "# Send an HTTP GET request to the website and parse it\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a1afdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_tags = soup.find('table', id='player_wages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0611ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html(str(table_tags))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48249fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekly Wages']=df['Weekly Wages'].str.extract(r'Â£ (\\d[\\d,]*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb6f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['Player','Weekly Wages']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75d3688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekly Wages'] = df['Weekly Wages'].str.replace(',', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "908633e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/player_wages.csv',index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23adfd",
   "metadata": {},
   "source": [
    "### Scraping Country Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753642bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://www.iban.com/country-codes\"\n",
    "\n",
    "# Send an HTTP GET request to the website and parse it\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_tags = soup.find_all('table')\n",
    "df = pd.read_html(str(table_tags))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab15012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('country_code.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b220d",
   "metadata": {},
   "source": [
    "### Scraping from Transfer Markt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8143ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website to scrape\n",
    "url = \"https://www.transfermarkt.com/premier-league/marktwerte/wettbewerb/GB1/pos//detailpos/0/altersklasse/alle/plus/1\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Send an HTTP GET request to the website and parse it\n",
    "response = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5064bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.transfermarkt.com/premier-league/marktwerte/wettbewerb/GB1/pos//detailpos/0/altersklasse/alle/plus/1/page/\"\n",
    "\n",
    "df_players = pd.DataFrame()\n",
    "\n",
    "for i in range(1,5):\n",
    "    # Send an HTTP GET request to the website\n",
    "    response = requests.get(url+str(i),headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table_tags = soup.find('table', class_='items')\n",
    "    df = pd.read_html(str(table_tags))[0]\n",
    "    df.dropna(subset=['#'], inplace=True)\n",
    "    df_players = pd.concat([df_players, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1664c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.to_csv('player_valuation.csv',index_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
